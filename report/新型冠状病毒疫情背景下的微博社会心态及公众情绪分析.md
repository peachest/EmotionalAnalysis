<h1 align = "center">新型冠状病毒疫情背景下的微博社会心态及公众情绪分析</h1>
<div align = "center" style = "font-size:1rem">侯雨希，叶俊濠，张耒</div>




[TOC]

## 研究思路

​	本实验以“肺炎”为关键词，爬取微博搜索网站的文本作为原始数据。对爬取下来的数据进行初步处理，包括：删除微博文本中的标题、时间、引用的tag、超链接，删除微博及其评论的文本中的一切非中文字符。对初步处理后的数据根据发布的时间划分进四个时间段中，对每个时间段的数据，一方面利用结巴分词对其进行分词操作得到分词集，通过TF-IDF方法对分词集进行权值计算，并提取权值最大的前50个词语作为该时间段中所有文本的特征词；另一方面，基于情感词词典与预先定义好的情感值计算公式计算特征词的情感强度，进而计算文本的情感值以及文本的情感倾向强度。再结合得到的情感倾向强度，计算每个文本下的特征词的权重，构建每个文本的情感特征向量，组成情感特征矩阵。最后再对该特征矩阵使用K-means聚类算法对数据进行聚类。对聚类结果采用PCA降维后进行可视化。研究思路框架图如图一所示：

<center><img src="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets/研究思路框架图.png" alt="图一" style="zoom:80%;"/></center>

<div align = "center">图一</div>

## 	实验过程

###	数据采集

#### 爬取微博数据

##### 数据内容

​	本文的数据来源为微博搜索网站（https://s.weibo.com/）。微博网站（https://weibo.com/）作为一个开放性的网络社区，拥有大量活跃的用户群体，是普通网民信息分享、传播以及获取的一个重要平台。对平台上的文本数据进行研究分析能够了解在不同背景下公众的观点态度以及情绪反应，因此微博已经成为意见挖掘和情感分析的重要资源[<sup>1</sup>](#ref1)。由于微博网站缺少高级搜索功能，本文因此使用微博搜索网站上的高级搜索功能，爬取符合条件的微博博文数据以及该博文下的评论数据，并选择忽略评论下的二级评论数据。

​	考虑到疫情初期，”2019新型冠状病毒“以及”新型冠状病毒肺炎“等正式名称并未提出与普及，因此本文以“肺炎“为关键字在微博网站上进行搜索。为了提高爬取数据的代表性，搜索时选取”热门“与”全部“标签。爬取数据的时间范围选择2020年1月1日至2020年6月30日，爬取天数的步长为1天，每天爬取1页数据，平均每页有20篇微博。爬取的内容包括微博的文本内容、发布时间、转发数、点赞数及评论数。对于每条微博下的评论，爬取10条评论的文本内容与点赞数。

##### 爬取技术

​	本文使用python语言，选择selenium自动化测试框架与Chrome驱动编写爬虫程序。该爬虫模拟用户操作chrome浏览器登录微博账户并进行数据爬取。利用selenium框架编写爬虫程序爬取微博数据的优点主要有二：

1. 可以忽略微博的反爬虫措施。网络上搜索得到的绝大部分现成的python微博爬虫程序，或者使用scrapy框架，或者直接使用request与response语句尝试连接微博服务器。但是在微博反爬措施逐年严格的情况下，经尝试，这些程序都已经过时，无法登录微博完成数去爬取任务。而采用selenium可以轻松登录微博，因为其工作方式是模拟用户对浏览器的操作。因此只需要程序正确填写账号密码与实现登录的各个步骤，即可登录微博账号，实现数据爬取。由于微博登录方式的限制，本文在爬取微博数据时无法完全实现自动化登录微博。解决方法是，在电脑端进行登录时，利用手机上的微博app扫一扫功能完成身份验证。登录程序的片段如下：

   ```python
   self.browser.get("https://s.weibo.com") # 跳转到微博搜索网站
   
   # 模拟点击登录
   self.browser.find_element_by_xpath('//*[@id="weibo_top_public"]/div/div/div[3]/div[2]/ul/li[3]/a').click()
   
   #输入账号、密码
   self.browser.find_element_by_xpath('//div[@class="item username input_wrap"]/input').send_keys(username)
   self.browser.find_element_by_xpath('//div[@class="item password input_wrap"]/input').send_keys(password)
   
   #点击确认登录，随后需要在微博app进行扫一扫登录
   self.browser.find_element_by_xpath('//div[@class="item_btn"]/a').click()
   ```

2. 程序直观，容易编写与调试。因为本次实验既需要爬取微博的博文又需要爬取博文下的评论，爬取逻辑复杂，若采用request与response获取页面页面相应，需要解析页面的JavaScript以及json再确定所需元素所在的位置，如果需要从头编写爬虫则花费时间长且程序不直观；调试时需要通过查看日志来寻找出错信息，十分困难。采用selenium框架，可以在程序运行时实时看到页面跳转，了解当前程序的进度，调试直观。而调用selenium现成的元素查找方法可以在不解析json的前提下直接使用用id、xpath等多种方式查找出所需元素，程序简短简单。爬虫爬取数据的核心程序片段如下：

	```python
flag = False
#爬取微博博文，同理可爬取微博的时间、点赞数、评论数、转发数
try:
   	text = self.browser.find_element_by_xpath(
        '//*[contains(@id, "Pl_Official_WeiboDetail")]'
        '/div[@node-type="feed_list"]/div[@node-type="feedconfig"]/div[@action-type="feed_list_item"]'
        '/div[@node-type="feed_content"]/div[@class="WB_detail"]/div[@node-type="feed_list_content"]').text
    	text = self.cleanBlogText(text) #清洗微博文本数据
    	flag = True
except selenium.common.exceptions.NoSuchElementException:
   print('第一次尝试查找text失败')
   ```
	```python
#爬取每个微博下至多10条评论
		validCommentNum = 0
    	#查找到所有的评论元素
        allRootCommentElements = self.browser.find_elements_by_xpath('//*[@node-type="root_comment"]')
        for rootCommentElement in allRootCommentElements:
            if (validCommentNum >= 10): break
                   
            #爬取评论的点赞数    
            commentLikeNum = rootCommentElement.find_element_by_xpath(
                './/*[@node-type="like_status"]/em[2]').text
            if (commentLikeNum == '赞'): commentLikeNum = 0;#当点赞数为0时，网站上的数据为中文字符“赞”，而不是‘0’
           
            #爬取评论的文本
            commentText = rootCommentElement.find_element_by_xpath(
                './div[@node-type="replywrap"]/div[@class="WB_text"]').text
            cleanedCommentText = self.cleanCommentText(commentText) #清洗评论文本            
            if (cleanedCommentText == None): continue
               
            validCommentNum += 1
            comment = {'text': cleanedCommentText, 'likeNum': int(commentLikeNum)}
            comments.append(comment)
   ```
#### 数据预处理

​	虽然爬取下来的数据包括了微博的以及其下的评论。但是由于二者都可能包含了其发布者的情绪，因此在后续分析中将所有微博与评论看作是同等的文本数据进行处理以及情感倾向预测。下列所有“文本”一词均代指微博的文本内容以及微博评论的文本内容。若需要将微博文本与评论文本分别处理，则会进行特别说明。

##### 文本清洗

​	由于本文数据来源为微博上的博文及其评论，因此针对微博博文与评论各自的特点分别采用不同的数据清洗方法，以提高后续分析与研究的效率。去除停用词等进一步数据处理将放在分词步骤中进行。

​	微博博文数据的预处理主要包括删除文本中无关内容，时间及日期，以及删除其余所有非中文字符。经过查看大量微博，总结出微博的博文中的特殊内容有如下几点：用中括号“【】”括起来的主题概括、用井号“##”括起来的微博tag、表示邀请某人查看该微博的“@”及其后续若干字符以及超链接。另外由于文本中出现的时间与日期不影响后续情感分析，因此选择一并删除。微博文本数据的清洗程序如下：

```python
    s = re.sub(r'【.*?】|#.*?#|（.*?）', '', str)  # 删除【】，（），##及其中的内容
    s = re.sub(r'([0-9]{1,4}[年月日])|以来|上午|下午|晚上|近期','',s)  # 删除日期
    s = re.sub(r'L.*', '', s)  # 删除微博超链接
    s = re.sub(r'(@.*?\s)', '', s)  # 删除@及其后的内容
    s = dataCleaner.findAllChineseCharacter(s)
```

​	每条博文下的评论的处理为删除文本中的用户名及其后的冒号，删除所有非中文字符。评论文本数据的清洗程序如下：
```python
	s = str[str.index('：') + 1:]  # 去掉评论中的用户名与‘：’
	s = dataCleaner.findAllChineseCharacter(s)
```
##### 文本分词

​	本文采用目前中文NLP研究中得到广泛应用的开源项目——“结巴分词”。结巴分词的原理是基于统计词典，先构造一个前缀词典，再利用前缀词典对输入句子进行切分，得到所有的切分可能，根据切分位置构造有向无环图；通过动态规划的方法计算得到图中具有最大概率的路径，再基于路径找出基于词频的最大切分组合。对于未登录词，采用了基于汉字成词能力的HMM模型，并运用Viterbi算法进行计算及标注词性[<sup>2</sup>](#ref2)。

​	为了提高后续步骤的准确性，加快后续计算的速度，在分词的过程中需要对样本集里频繁出现且分布均匀的、类别信息量小的停用词进行剔除。本文采用查表法进行去停用词，将完成分词后的文本与停用表也就是停用词词典进行匹配并删除停用词。本文选取CSDN论坛上现成的停用词词典作为基础中文停用词词典（由于疏忽没有记录具体链接），在已有词典上对数据进行TF-IDF特征词提取，人工筛选出不符合本次实验要求的停用词并添加进停用词典，反复迭代，以达到对新冠疫情背景下的评论语料进行情感分类有更优的效果。筛选出的停用词主要包括：“北京”、“中国”、“美国”等地名，“人员”、“外国人”等描述人的词语。所有补充的停用词见下表：

|      |      |        |        |        |          |        |
| :--: | :--: | :----: | :----: | :----: | :------: | :----: |
| 一个 | 时间 |  累计  |  真的  |  病例  |   中国   |  美国  |
| 截至 | 新闻 |  新型  |  工作  |  人员  |   武汉   |  日本  |
| 人数 | 很多 |  香港  |  华南  | 卫健委 | 医护人员 | 意大利 |
| 伊朗 | 韩国 | 西班牙 | 外国人 |  全球  |   北京   |        |

#### 划分时间段

​	面对新冠肺炎这种新发重大传染病，不同的时间段中根据政府不同的应对策略公众的情绪将有相应的反应，因此需要根据中国抗疫的阶段将数据进行分类。中国根据疫情发展态势和风险，不断优化调整防控策略。整体来看，面对疫情中国应对策略可分为四个阶段[<sup>3</sup>](#ref3)：  

  1.  2019年底至2020年1月20日，面对未知病毒，加强信息收集、分享和研判，卫生部门组织应对。  

  2.  1 月 21 日至 2 月 23 日，估测疫情向全国加速扩散的风险，动员全社会力量，实施严格防控和集中救治。  

  3.  2月24 日到 3 月中下旬，全国疫情高发风险得以控制，统筹推进科学防控和有序复工复产。  

  4.  3月下旬以来，国内本土传播基本阻断，境外风险陡增，实行“内防反弹，外防输入”的防控策略。  

​	因此按照上述时间点，本文将所有微博数据按照发布的时间划分成四类，对每一类分别进行后续的数据处理。划分的四个时间段分别为（年份均为2020年）：

|    时间段1     |     时间段2     |     时间段3     |    时间段4     |
| :------------: | :-------------: | :-------------: | :------------: |
| 1月1日~1月20日 | 1月21日~2月23日 | 2月24日~3月31日 | 4月1日~6月30日 |

#### 微博热度计算

​	不同微博的热度存在差异，衡量某条微博的热度需要通过该微博的附加信息进行计算。通过小组讨论以及从一般经验上考虑，选取微博的“点赞数”、“评论数”与“转发数”这三个属性来表征某条微博的热度。本文选择直接利用三个属性之和来计算某条微博的热度，没有通过更换热度计算公式来比较对结果的影响。计算公式如下：
$$
\large hot_{blog}=like\_num_{blog}+comment\_num_{blog}+forward\_num_{blog}
$$
其中$hot_{blog}$表示微博$blog$的热度，$like\_num_{blog}$、$comment\_num_{blog}$、$forward\_num_{blog}$分别表示微博$blog$的点赞数、评论数与转发数。

###  微博文本情感向量构建

​	计算机无法直接处理文字等非结构化的数据，因此我们需要将爬取下来的文本转化为计算机能够识别的结构化数据，这一过程称为文本表示，文本表示是计算机理解人类语言的桥梁[。本文选择one-hot的数值向量形式表征分词后的离散文本的特征，为了避免维数灾难，对分词集进行TF-IDF特征提取，选取TF-IDF值最大的前50个词语作为所有文本的特征词。

​	若仅采用TF-IDF寻找特征词将忽略文本的语义信息，则无法保留不同词之间的关系。因此在计算文本的情感向量时引入情感倾向强度ISO概念，采用改进后的STF-IDF计算特征词的权重，从统计学角度和情感语义角度两方面衡量，计算每个文本下特征词的权值并由此得出特征文本的向量[<sup>4</sup>](#ref4)。

#### 文本特征提取

​	TF-IDF（Term Frequency-Inverse Document Frequency）是一种评估某个词语对于某个文件集或一个语料库中的一份文件的重要程度的统计方法，通常用于资讯检索与资讯勘探。TF-IDF方法倾向于过滤掉常见的词语，保留重要的词语，从而实现特征降维[<sup>5</sup>](#ref5)。

​	在一份给定的文件集里，词频TF指某一个给定的词语在文件集中出现的频数。由于同一个词语在不同长度的文章中出现的次数不一样，较长的文章出现的频率可能越高，为了防止TF偏向较长的文件，需要对TF进行归一化，计算公式如下：
$$
\Large TF_{i,j,k}=\frac{n_{i,j,k}}{\sum\limits_{K_{i,j}} n_{i,j,k}}
$$
其中$d_{i,j}$表示第i个时间段中的第j个文件，$TF_{i,j,k}$即词$t_k$对文件$d_{i,j}$的词频，公式中的分子$n_{i,j,k}$是词$t_k$在文件$d_{i,j}$中出现的次数，分母则是在文件$d_{i,j}$中所有字词出现的次数之和。

​	逆文档频率IDF是一个词语普遍重要性的度量。在词频的基础上对每个词语赋予一个权重，如果一个词在大量文件中均有出现，则对当前文件的代表性较低，应赋予该词较小的权重；反之代表性高，权重较大。计算公式如下:  
$$
\Large IDF_{k} = log\frac{D}{1+(J:t_k\in d_{i,j})}
$$
其中$IDF_{k}$即特定词语$t_k$的IDF值，$D$表示文件集中的文件总数，$(J:t_k\in d_{i,j})$为包含词语$t_k$的文件数目。如果该词不在文件集中，会导致$(J:t_k\in d_{i,j})$为零，因此一般情况下使用$1+(J:t_k\in d_{i,j})$作为分母。

​	TF-IDF技术的核心的思想是，如果某个词语在所有文件中出现的频率较低，即逆文档频率IDF高，说明该词语具有很好的类别区分能力；而如果该词语在某类文件中出现频率较高，即词频TF高，说明该词语能够很好代表这个类的文本的特征，这样的词可以选来作为该类文本的特征词以区别其他类的文档。因此对于在某一特定文件里的词语，其TF-IDF值越高，可以认为其对当前文件的重要性越高，特征表征能力更强。TF-IDF值计算方式如下：
$$
\Large TF-IDF_{k} = {TF_{i,j,k}}*{IDF_{k}}
$$

​	本文选用TF-IDF方法，对4个时间段的文本在去除停用词与分词的基础上，分别进行特征词提取。每个时间段都选择TF-IDF值最大的前50个词作为该时间段中文本集的特征词集$feature\_words_{i}=\{fw_{i,1},fw_{i,2},…,fw_{i,50}\}(i=1,2,3,4)$。每个时间段的特征词提取结果制作成词云按时间线展示如下图:
<iframe  height=520px, src= "新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\特征词词云.html"></iframe>



#### 文本情感分析

​		为了构建文本的情感特征向量，需要对文本进行情感分析。本文认为每个文本的情感值$sentiment-value$由文本的基础权重$Basic-value$以及各个特征词的情感强度$v_k$所决定。文本的情感值正负代表了不同的情感倾向，正情感值代表积极的正情感倾向，反之负情感值代表负情感倾向，而零代表中性情感倾向。进而得出文本的情感倾向强度$ISO$。

##### 特征词的情感强度

​	本文先将特征词的情感倾向分为正向（积极），负向（消极）以及中性三种赋予其基础情感值。利用现成的情感词典，包括：积极词词典（正面评价词语、正面情感词）以及消极词词典（负面评价词语、负面情感词）对特征词进行匹配，若匹配成功认为其具有相应的情感倾向，获得基础情感值。其中，积极词典具有+1的基础情感值，消极词典则具有-1的基础情感值。否则认为其具有中性的情感倾向，获得0的基础情感值[<sup>6</sup>](#ref6)。特征词$t_k$的基础情感值如下：
$$
base_k=\left\{\begin{array}{}
		+1 \quad 正向\\
		0 \quad 中性\\
		-1 \quad 负向\\
		\end{array}
		\right.
$$


​	特征词的情感强度还涉及到程度强弱的问题，因此我们引入程度副词。程度副词的出现能够增强或减弱情感强度，而根据不同的程度副词修饰的情况，还需要视程度词的级别分别增强或减弱情感强度[<sup>7</sup>](#ref7)。本文使用了现成的6种程度副词表匹配程度副词，并使用附带的现成权重值计算权重。某程度副词的权重设置如下：
$$
adv = 
\left\{
	\begin{array}{}
	0.5 \quad insufficiently\\
	0.8 \quad little\\
	1.2 \quad more\\
	1.25 \ \  very\\
	2  \quad \ \ \  most\\
	\end{array}
\right.
$$
​	文本中否定词的出现将直接将特征词的原有的情感倾向转向相反的倾向，而且通常效用是叠加的[<sup>8</sup>](#ref8)。本文采用最简单的否定词权重算法，权重设定如下:


$$
neg=(-1)^{N}
$$
其中N为否定词的个数。

​	则某个特征词的情感强度可以通过以下公式计算[<sup>9</sup>](#ref9)：
$$
\Large v_{k,\ d_{i,j}}=base_k*\prod adv_{pre}*\prod neg_{pre}
$$
其中$d_{i,j}$表示第i个时间段中的第j个文件，$v_{k,\ d_{i,j}}$为特征词$t_k$文件$d_{i,j}$下的情感强度；$\prod adv_{pre}$为文件中，位于前一个特征词之后，该特征词之前的所有副词权重之积。同理定义，$\prod neg_{pre}$为文件中，位于前一个特征词之后，该特征词之前的所有否定词权重之积。具体程序片段如下：

```python
        value = 0 #文本情绪值
    	count = 0 #否定词的计数器
        rate = []
        for i in l:    #累计某个情绪词前的副词与否定词，
            if (isHit(i,not_word)):
                count += 1
            elif (isHit(i,most)):
                rate.append(2)
            elif (i in more):
                rate.append(1.2)
            elif (i in very):
                rate.append(1.25)
            elif (i in little):
                rate.append(0.8)
            elif (i in insufficiently):
                rate.append(0.5)
```
##### 文本情感值计算

​	 每个文本可能包含若干个特征词$t_k$，因此我们认为可以用文本包含的特征词的情感强度之和表征文本的情感值。因此，文本的情感值计算公式如下：
$$
\Large s_{d_{i,j}} = \sum\limits_{(k\ : \  t_k\  \in\  d_{i,j})}{v_{k,\ d_{i,j}}}
$$
其中$d_{i,j}$表示第i个时间段中的第j个文件，$s_{d_{i,j}}为文件$$d_{i,j}$的情感值，$\large v_{k,\ d_{i,j}}$为出现在该文本中的特征词$t_k$在该文本下的情感强度。计算的程序片段如下：

~~~python
#接上一个程序块    			
                elif (isHit(i,pos)): #特征词是积极词
                    num = 1
                    for j in rate:
                        num = num * j #根据副词的类别加权
                    rate = []
                    if (count % 2 == 1):
                        num = 0.0-num #根据否定词的数量对情感倾向取反
                        count = 0
                    value += num #累加特征词的情感强度
                elif (isHit(i,neg)): #特征词是消极词
                    num = 1
                    for j in rate:
                        num = num * j #根据副词的类别加权
                    rate = []
                    if (count % 2 == 1):
                        num = 0.0-num #根据否定词的数量对情感倾向取反
                        count = 0
                    value -= num #累加特征词的情感强度
~~~

​	本文参考文献[<sup>10</sup>](#ref10)引入情感倾向强度ISO对每个独立的文本进行情感值加权计算，运用数据分析的方法，从数学的角度研究公众在新冠疫情的背景下表达情感的倾向。文献中原有的方法将每个文本的基础权重设置为1，并结合特征情感词、程度副词与否定词计算得出文本最终的情感值。经过小组多次实践、研究及讨论，我们提出了基于微博的附加信息的改进的计算公式。核心思想是：不同微博的热度存在差异，更高的热度说明公众对该文本表达的情感倾向表示出更强烈的认同感，反之低热度文本的情感更难表征公众的情感倾向；另外，如果某条评论的情感倾向与其所属的微博的情感倾向相同，应该加强评论的情感倾向，反之应该进行削弱。因此根据微博文本的热度对其评论给予不同的权重，这将有效增强情感聚类的效果。对某条微博$blog$热度的表征本文采用了微博文本附带的“点赞数”、“评论数”与“转发数”这三个属性之和，并已计算为$hot_{blog}$。权重的设置如下：
$$
weight_{blog}=\left\{
                \begin{array}{}
                  1 \quad\ \ \  0\leq hot_{blog} < 3000\\
                  1.1 \quad 3000\leq hot_{blog} < 3,0000\\
                  1.2 \quad 3,0000\leq hot_{blog} < 10,0000\\
                  1.3 \quad 10,0000\leq hot_{blog} < 100,0000\\                  
                  1.4 \quad 100,0000\leq hot_{blog}\\
                \end{array}
              \right.
$$
其中$weight_{blog}$表示根据某条微博$blog$的热度赋予的权重。权重计算的片段如下：

```python
        if("万" in blogLikeNum) or("万" in blogForwardNum) or("万" in blogCommentNum): #数据过百万会显示成‘100万+’
            return '1.4'
        if("评论" in blogCommentNum) or ("转发" in blogForwardNum) or ("赞" in blogLikeNum): #有部分评论，转发，赞存储了对应的字符串
            return '1'
        if(sum<3000) and (sum>=0):
            return '1'
        elif (sum>=3000) and(sum<30000):
            return '1.1'
        elif (sum>=30000) and (sum<100000):
            return '1.2'
        elif sum>=100000:
            return '1.3'
```

​	用每天微博的权重对其下评论的情感值进行相应的加权，计算公式如下：
$$
\Large s\_weighted_{d_{i,j}} = \left\{
                \begin{array}{}
                  s_{d_{i,j}} * weight_{blog_i} \quad \\
                  \frac {s_{d_{i,j}}} {weight_{blog_i}} \quad \\
                \end{array}
              \right.(d_{i,j} \in blog_i\_comments)
$$
其中$d_{i,j}$表示第i个时间段中的第j个文档，$s\_weighted_{d_{i,j}}$表示文档$d_{i,j}$的情感值$s_{d_{i,j}}$用权重$weight_{blog_i}$加权后的情感值，且文档$d_{i,j}$属于微博$blog_i$的评论$blog_i\_comments$。程序片段如下：

```python
	if (float(head) * float(i) >= 0):          
          #微博内容情绪与评论相同，对评论情感加强
		temp = float(weightValue) * float(i)
		file.write(str(temp) + '\n')
        
	else: #两者相反，对评论情感进行削弱
		temp = float(i) / float(weightValue)
		file.write(str(temp) + '\n')
```

​	因此根据文本的具体内容，计算得到的情感值也有正负之分，因此可以根据值的正负将文本分为正向，负向与中性三种情感倾向。本文所得数据分时间段的情感值按时间线展示如下图：

<iframe height=540px, src = "新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论情感值统计.html"></iframe>

##### 文本的情感倾向强度

​	由于不同倾向的情感值分布可能不均匀，因此不能直接利用情感值构建文本的情感特征向量，而是需要计算情感倾向强度，对情感值进行归一化处理。本文参考文献，定义情感倾向强度（Intensity of sentiment orientation ISO）为当前文本的情感值与同一情感倾向中最大情感值的比例[<sup>11</sup>](#ref11)。ISO的计算公式如下：
$$
\Large ISO_{d_{i,j}} = \frac{s\_weighted_{d_{i,j}}}{MAX}
$$

其中$d_{i,j}$表示第i个时间段中的第j个文档，$ISO_{d_{i,j}}$为文本${d_{i,j}}$的情感倾向强度，MAX表示同一情感倾向的所有文本中情感值绝对值最大的情感值。由于情感有正负，因此正向的MAX即为最大的情感值，负向的MAX即为最小的情感值，零的情感值具有零的情感倾向强度。程序片段如下：

```python
for line in lines:
    l = line.split()
    for i in l:
        value = float(i) #字符转为数字
        if (value > 0):	#情感倾向为正
            value=value/max
        elif (value < 0): #情感倾向为负
            value=value/min
            value=0.0-value  #为了方便后续计算，将文本的情感倾向（正负号）附加到文本ISO的计算值上，
        else:
            value=0 #无情感倾向
```

#### 情感特征向量构建

​	为了使文本的情感转化为计算机能识别的文本，需要将特征词在每个文本下的权值对该文本进行one-hot编码。设置文本的特征词权值时，利用STF-IDF方法从统计学与情感语义两个角度进行计算。每个文本的情感特征以向量形式表示为：

$$
\Large T_{i,j}=(w_{i,j,1},w_{i,j,2},……,w_{i,j,50})
$$
其中$w_{i,j,k}$表示文本$d_{i,j}$的第$k\ \ (k = 1,2,…,50)$个特征词$fw_{i,k}$的在该文本下的权值。特征词在某个文本中的权值由该特征词的固有的情感倾向以及计算得出的文本情感倾向共同决定。特征词的权值在不同情况下有两种计算公式[<sup>12</sup>](#ref12)。

​	当文本$d_{i,j}$第$k$个特征词的情感倾向与文本相同时，权值的计算公式为：
$$
\Large w_{i,j,k}=（1+ISO_{d_{i,j}}）*TF-IDF_k
$$
​	当文本$d_{i,j}$中第$k$个特征词的情感倾向与文本不同或无情感倾向时，权值的计算公式为：
$$
\Large w_{i,j,k}=TF-IDF_k
$$

~~~python
    					for i in comment: #遍历评论的每一个词
                            if (i == pair1['高频词']): #如果是特征词，计算在该文本中下的权值
                                isPos = isHit(pair1['高频词'], pos)
                                isNeg = isHit(pair1['高频词'], neg)
                                ISO = float(pair2['ISO'])
                                #这里的ISO变量中同时储存了文本的ISO值以及文本的情绪倾向（正负）
                                TF_IDF = float(pair1['TF-IDF'])
                                if isPos:
                                    if (ISO > 0):
                                        num = (1.0 + ISO) * TF_IDF
                                    else:
                                        num = TF_IDF
                                elif isNeg:
                                    if (ISO < 0):    
                                        ISO = 0.0-ISO
                                        num = (1 + ISO) * TF_IDF
                                        num = 0.0-num
                                    else:
                                        num = 0.0-TF_IDF
                                else:
                                    num = TF_IDF
~~~

###  文本聚类

####  k-means聚类算法

​	K-means，全称为“k均值聚类算法”，是一种通过迭代求解的聚类分析算法。在讨论该聚类算法的具体实现逻辑之前，首先需要明确“聚类”和“分类”的联系和区别。聚类和分类都是将数据分类，但是分类的目标性强于聚类。分类是从特定数据中挖掘模式，并作出判断的过程。在对样本进行分类时，需要明确具体的目标，通过人为规定每一类应该具有的特征，进而根据分类目标进行归类。而聚类与分类的不同之处就在于聚类的目标并没有在一开始被定义出来，因此在聚类结论出来之前，我们完全不知道每一类有什么特征。对聚类的结果必须基于人的经验来进行分析，观察分成的类别中各自具有何种属性。聚类算法的核心思想是根据数据在空间分布上的聚集属性对数据进行归类，数据的实际意义并不影响聚类的结果。本文使用的k-means算法属于聚类算法的一种，其在预先规定数据应该分成几类的情况下对数据进行聚类，具体步骤如下：

1. 规定聚类结果的数目k，即希望将数据集经过聚类后得到k个分组
2. 从数据集中随机选择k个点作为初始的质心

	```python
	# 使用随机样例初始化质心
	
	def initCentroids(dataSet,k):
	    # k是指用户设定的k个种子点
	    # dataSet - 此处为mat对象
	    numSamples, dim = dataSet.shape
	    # numSample - 行，此处代表数据集数量  dim - 列，此处代表维度，例如只有xy轴的，dim=2
	    centroids = zeros((k, dim))  # 产生k行，dim列零矩阵
	    for i in range(k):
	        index = int(random.uniform(0, numSamples))  # 给出一个服从均匀分布的在0~numSamples之间的整数
	        centroids[i, :] = dataSet[index, :]  # 第index行作为种子点（质心）
	    return centroids
	```

3. 重复下列步骤：
   1. 对集合中的每一个数据点，计算其与k个质心的距离，将其分配到距离最近的质心所在的分组

      ```python
      # 计算欧式距离
      def euclDistance(vector1,vector2):
          return sqrt(sum(pow(vector2-vector1, 2)))  # pow()是自带函数
      
      ##find the centroid who is closest
      for j in range(k):
          if len(centroids[j]) == 0 or len(dataSet[i]) == 0:
              continue
          distance = euclDistance(centroids[j, :], dataSet[i, :])  # 计算到数据的欧式距离
          if distance < minDist:  # 如果距离小于当前最小距离
           minDist = distance  # 则最小距离更新
              minIndex = j  # 对应的点群也会更新
      ```
      
   2. 对所得k组数据，分别根据各组内包含的数据计算出各组的中心点，得到k个新的质心

      ```python
   ##update centroids
   for j in range(k):
       pointsInCluster = dataSet[nonzero(clusterAssment[:,0].A == j)[0]]  # 取列
       # nonzeros返回的是矩阵中非零的元素的[行号]和[列号]
       # .A是将mat对象转为array
       # 将所有等于当前点群j的，赋给pointsInCluster，之后计算该点群新的中心
       if len(pointsInCluster) == 0:
           continue
       centroids[j, :] = mean(pointsInCluster, axis=0)  #  最后结果为两列，每一列为对应维的算术平方值
      ```
   
   3. 若每一组中新的质心都与原来质心的距离小于某一个阈值，则算法结束
4. 最终得到k组数据集以及代表该组数据的中心点。

​	具体实现中，本文通过随机生成的方法在给定数据集中确定初始的k个质心，并采用欧拉距离作为数据点之间的距离的计算方式。本文在实验过程中进行了多次k-means聚类，发现在原始数据以及参数k不变的情况下，最终的聚类结果仍会发生波动，有时甚至出现所有数据点被分配到同一类的情况。通过查阅文献[<sup>13</sup>](#ref13)发现，由于传统的汇聚层次聚类算法具有不能很好地处理各类别密度分布不平衡的数据集的缺点，即如果类边缘点或者噪声点成为初始类中心点将使得生成的类簇与实际类别相比发生较大的整体偏移。因此K-means聚类效果在很大程度上取决于该过程的起始状态, 也就是初始类中心点的选择，并且算法并非终止于总体最优结果而是一个局部最优结果。而对于所有数据分配到一类的情况，本文分析原因为本次实验的数据在高维空间中呈现同心球式的分布，此时只需重新聚类，即可得到按距球心距离进行粗略分类的数据。

​	k-means算法中判定新质心与原质心的距离是否小于某一阈值这一步骤，本质上是为了判定数据的聚类结果是个否稳定，如果聚类结果稳定，则跳出循环。因此只要若能够直接确定数据聚类结果是否稳定，则不需要分析质心的距离。本文由于数据的值偏小，对阈值的确定存在困难，因此采用了最简单的决定聚类结果已经稳定的方法：即通过判定重新分配分组后数据的分组情况是否发生变化来判定聚类是否稳定，若分组情况不变，则聚类结果稳定，此时可跳出循环。

​	进行K-means聚类时还存在着一个难点，即对k值的确定。由于对超高维的数据分布缺乏整体的感知，因此k值的确定需要不断尝试，才能获得更好的聚类结果。本项目中，小组尝试了3、4、5三种k值，发现在前三个时间段中，$k>3$时聚类结果呈现出某两个中心点之间的距离远小于其与其它中心点之间的距离，这说明k-means聚类出现了过拟合的情况，而实际的高维数据空间中的相对密集区域并没有超过3个；而对第四个时间段，$k<4$时聚类结果出现了欠拟合的情况。因此本文决定将k-means算法中的k值设定为3，并额外对第四个时间段的数据进行k=4的聚类，最终取得了良好的聚类结果。四个时间段的聚类结果按时间线分别呈现如下：

<iframe  height=500px, src = "新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\聚类结果.html"></iframe>
<center>按时间线的聚类结果</center><br/>
<iframe  height=520]px, src = "新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\补充聚类结果.html"></iframe><center>补充的第四个时间段的聚类结果</center>

####  聚类结果

##### PCA降维

​	本文使用PCA的主要目的是进行K-means聚类结果的散点图绘制。散点图的绘制需要挑选高维数据中特征最突出、区分度最高的几个维度，确保获得更好的可视化效果。本文采用python的sklearn库中的PCA接口对数据进行降维，挑选出最具区分度的3个维度进行绘图。

​	PCA(最大化方差定义或者最小化投影误差定义)是一种无监督算法，可以再不需要标注标签的条件下对数据进行降维操作，应用范围十分广泛。其核心思维是：

* 对于高维变量构成的数据集，PCA的目标是将数据投影到维度较低的子空间中，同时最大化投影数据的方差
* 将原始数据投影到低维子空间时，选择k(低维子空间的维数)个单位正交基，使得原始数据变换到这组基上后，各变量两两之间的协方差为0(使得各变量之间相关系数为0)，而变量自身方差尽可能大

​	采用PCA主成分分析法，能够缓解维度灾难，使得可视化绘图成为可能。同时PCA降低了数据噪声，在绘图中反映出数据的主要特征，同时避免了不同聚类的数据交错出现在绘图中的情况。但是PCA也存在着一些缺陷，譬如将高维数据投影到低维空间时得到的数据的实际意义并不明确，但本文使用PCA降维的目的在于可视化数据聚类结果，不需要考虑数据实际意义；同时，PCA会导致数据过拟合，忽略一些看似无用的关键信息，本文使用PCA降维后的数据绘制出的散点图近似分布于一个平面上，这很有可能是过拟合产生的影响。

##### 结果可视化

​	本文中对四个时间段的数据进行k-means聚类以及PCA降维后数据的可视化结果如下列三维散点图所示，每幅散点图都可以用鼠标滑轮缩放以及鼠标拖动以从不同角度查看数据点在三维空间中的分布情况：

<iframe  height=600px, src = "新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\聚类结果可视化.html"></iframe>

若将时间段4（4月1日至6月30日）的数据聚成3类，则结果无法满足小组对后续人工归纳情绪倾向的要求，因此经过研究，小组决定将该时间段的数据聚成4类，并保留聚成3类的结果一并展示。

## 实验结果

### 结果展示

​	通过上述各个实验步骤，我们小组结合中国政府防疫政策，基于每个事件段提取的特征词以及根据最终的聚类结果，通过人工分析归纳了每一类微博文本的情感倾向，初步明确了公众在病毒爆发时期的行为规律以及情感倾向变化。人工分析时发现聚类效果并不完美，消极的类别中也可能包含明显是积极的评论。同时经过反复实验，将第四个时间段的数据聚成4类能更好的解释公众情绪及行为规律。各时间段下聚类的情感倾向归纳如下表，鼠标单击链接可查看具体的评论内容：

| 时间段          | 积极                                                         | 中性                                                         | 消极                                                         |
| --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1月1日~1月20日  | <a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time01_comment_category_03.html">类别03</a> | <a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time01_comment_category_02.html">类别02</a> | <a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time01_comment_category_01.html">类别01</a> |
| 1月21日~2月23日 | <a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time02_comment_category_01.html">类别01</a> | <a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time02_comment_category_02.html">类别02</a> | <a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time02_comment_category_03.html">类别03</a> |
| 2月24日~3月31日 | <a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time03_comment_category_01.html">类别01</a> | <a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time03_comment_category_02.html">类别02</a> | <a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time03_comment_category_03.html">类别03</a> |
| 4月1日~6月30日  | (<a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time04_comment_category_01(k=3).html">类别01(k=3)</a>)<br/><a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time04_comment_category_01(k=4).html">类别01</a> | (<a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time04_comment_category_03(k=3).html">类别03(k=3)</a>)<br/><a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time04_comment_category_02(k=4).html">类别02</a><br/><a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time04_comment_category_03(k=4).html">类别03</a> | (<a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time04_comment_category_02(k=3).html">类别02(k=3)[主题不明确]</a>)<br/><a name="jump" href="新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\评论分类结果\time04_comment_category_04(k=4).html">类别04</a> |

将各类情感的评论数量依照百分比绘制成折线图展示如下：
<iframe  height=520px, src = "新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\实验结果(k=3).html"></iframe>
<center>结果聚成3类的折线图</center><br/><span>时间段4的第三种类别主题不明确，无法简单归为消极。因此在折线图中除去该类数据</span><br/><br/><div></div>



<iframe  height=520px, src = "新型冠状病毒疫情背景下的微博社会心态及公众情绪分析.assets\实验结果(k=4).html"></iframe>
<center>结果聚成4类的折线图</center>

### 结果分析

​	在1月1日至1月20日的第一时间段中，肺炎疫情爆发的原因被错误地归纳为部分地区吃野味的习俗，引起社会公众对野生动物携带病原体的忧虑。同时由于疫情信息从医院上报至各级疾控处的程序以及政府的决策过程需要一定的时间，未能及时缓解公众对实时疫情信息需要的焦虑情绪，一定程度上引发公众的恐慌。因此在聚类结果中，类别01的情绪被我们小组归纳为负面，评论内容主要集中在：对吃野生动物的行为的愤怒与谴责、对未知的新型传染病的恐惧以及对相关部门信息不公开的不满。在这一时间段中，由于政府开始逐步加强了对疫情信息的收集、分享和研判，政府卫生部门也开始组织应对，“不信谣，不传谣”、对食品安全的重视以及个人防护的注意成为类别02中主流的中性观点。另外，评论中也出现了以戴口罩、勤洗手、保持环境等为主要内容的积极的防范疫情的措施。

​	而在1月21日至2月23日的第二时间段中，我国的疫情及病例信息已经基本透明公开。1月23日武汉正式封城，全国开始实施严格防控和集中救治，动员全社会的力量支援武汉。词云中的“隔离”、“支援”、“捐赠”、“抗击”等特征词可以印证上述防疫情况。该时间段中，类别01的积极评论的内容主要集中在对前线人员的鼓励和加油，表达出对武汉以及国家能够打赢这场战役的坚定信心，同时有一部分是宣传科学防疫的中性评论。对于该时间段中类别03的消极评论数量相对上一时间段的爆发式上升，经小组分析，原因是该时间段中，公众对疫情的重视程度上升到了一个顶峰，而一省对一市的举国支援武汉封城之举更是激发了全国人民的集体意识。在这种状况下，个别利欲熏心的商贩贩卖假冒伪劣医疗用品，以及武汉红十字会物资分配不透明等种种妨碍抗疫的行为触动了公众的底线，大众对这种发国难财的行为进行了严厉的谴责。同时，国内媒体的报道都集中在各地的抗疫措施中，各种网络平台也不断公布新的疫情信息，抗疫过程中的乱象得到更多扩散，这也是导致消极评论数量上升的原因之一。另外，由于疫情初期，公众对病毒来源的认知受到误导，因此仍存在一部分谴责吃野生动物的消极评论。
​	在2月24日至3月31日的第三时间段中。全国疫情高发风险得以控制，开始统筹推进科学防控和有序复工复产。因此类别01中公众对工作在抗疫一线的工作人员表达了强烈的感谢与致敬之情，对不幸感染上新型冠状病毒的患者表达尽快康复、战胜病魔的祝福。也有部分评论是希望疫情尽快结束的中性评论。而被分类为消极评论的类别03的数据数量出乎了我们小组的意料，因为实验前我们小组猜想消极评论的数量应该先上升后下降。经过小组成员仔细查看该分类中的评论内容，对该类数据量居高的现象归纳出2点原因：

1. 部分消极评论并非针对此次疫情。经过人工查阅分类内容，发现部分消极评论针对的是《中华人民共和国外国人永久居留管理条例（征求意见稿）》（下称《条例》）[<sup>14</sup>](#ref14)，而在词云中也能找到“永久”这一相关的特征词。经查证，2020年2月27日0时，中华人民共和国司法部以及中国政府法制信息网（http://www.moj.gov.cn/）发布了《条例》的意见稿。公众对该意见稿爆发激烈的争论，主要围绕该《条例》有关资格、条件的设计是否合理，会不会出现大量境外人员挤占国内就业岗位和社会公共福利资源问题，也有的担心一些规定过于原则、不够细化，顾虑实施后出现管理漏洞[<sup>15</sup>](#ref15)。
2. 消极评论针对外国。公众的负面情绪开始从对内转向对外，对外国的虚伪、不人道行为表示斥责。以美国为首的一批西方政要和媒体，出于掩饰本国抗疫沦陷等政治原因或偏见，对中国的抗疫行为极力诬陷、丑化。他们将新冠病毒污名为“中国病毒”或“武汉病毒"，造谣 “病毒来自中国实验室；”污蔑“中国信息不透明”，在疫情数据上造假；把病毒在全球暴发的责任归之于中国 ，谎话连篇[<sup>16</sup>](#ref16)。而针对武汉封城的行为则大肆宣扬中国“侵犯”人权，认为中国抗疫需要的是西式的“自由”、“民主”。但是这些西方国家面对疫情时的表现却不尽人意，引发国内民众的不屑与嘲笑。经查看，部分消极评论集中在对美、英、法、巴西等国家的抗疫措施。

​	最后一个时间段为4月1日至6月31日，国内本土传播基本阻断，境外风险陡增，开始实行“内防反弹，外防输入”的防控策略。  
​	将数据聚类为3类时，人工查看发现类别02下的评论包含了各种主题的评论，并不能准确的概括出该类的情感倾向；反而在被归纳为积极的类别01以及归纳为中性的类别03中的评论主题非常集中。分类01中的评论基本都包括了“加油”或者“特朗普”这两个关键词，经确认，这两个词语并不会同时出现在同一个评论中，推测分别表示了公众对各类前线人员的加油，以及对特朗普2020年大选的关注。类别03中的评论关键词是“疫情”，内容也全部集中在对疫情的讨论。  
​	将数据聚类成4类时，聚类结果得到较大的改善。经查看，01类别下的评论均包含了“加油”的关键词，主要的内容包括对祖国抗疫的鼓励，对俄罗斯以及非洲等与我国交好的国家与地区等抗击疫情的加油打气。02类别的评论以“疫情”为关键词，03类别的评论以“口罩”为关键词，大部分评论以阐述事实为主要内容，因此小组认为应将这两个类别一并认定为属于中性的情感倾向。04类别下的部分评论表达了对以美国为代表的西方国家消极抗疫的态度以及对以特朗普为代表的西方政客跳梁小丑般的行径耻笑，主题是对国外疫情的消极看法。  
​	对于为什么聚成3类的结果不如4类的问题，经过小组讨论，认为是由于该时间段的数据中，包含“疫情”、“口罩”、“加油”等词语的评论居多，容易聚成一大类。因此当设定聚集成3类时，包含“加油”或“疫情”关键词的评论分别被聚集成一类，其余评论全被归为剩下的一类，因此造成该类大量评论数据混杂，聚类主题不明确。当聚集成4类时，包含“疫情”、“口罩”与“加油”关键词的评论被归纳为单独的一类，则将剩余体现消极情绪的评论归为一类。因此将最后一个时间段的数据聚成4类，能提高人工归纳的准确性。

## 结语

​	当下微博以实时快捷的个性化短文作为特点，已经成为最受欢迎的社交网络平台。突发公共卫生事件发生时，公众倾向于从微博上获取消息以及发表感受。对微博上数据进行情感分析有助于研究当前用户对突发公共卫生事件的情感状况，进行网络舆情分析，为舆情的识别以及预测提供参考。本文参考各类文献，提出一个改进后的方案，并对微博的数据实现了在新冠病毒背景下的微博社会心态及公众情绪分析。

## 参考文献

[1]<a id = 'ref1' href = 'https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2019&filename=JSJY2018S2019&v=aKeECELjBbK%25mmd2FGP7XeXvraq3CGRp%25mmd2B64sjqf7mtz2WkWTTLKVisQthCfYtNT%25mmd2BC3evF'>郝苗苗,徐秀娟,于红,赵小薇,许真珍.基于中文微博的情绪分类与预测算法[J].计算机应用,2018,38(S2):89-96.</a>

[2]<a id = 'ref2' href = "https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2019&filename=XXDL201918017&v=8T1%25mmd2BB4hMCPB8cMDCStYbPFRzWGlFVBWBbc9gkgiF89uKLMm%25mmd2BR9FgRQfjDhuvaRh0">曾小芹.基于Python的中文结巴分词技术实现[J].信息与电脑(理论版),2019,31(18):38-39+42.</a>

[3]<a id = "ref3" href = 'http://www.cikd.org/chinese/detail?leafid=212&docid=1429'>http://www.cikd.org/chinese/detail?leafid=212&docid=1429</a>

[4]<a id = 'ref4' herf = "https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CAPJ&dbname=CAPJLAST&filename=XDTQ20201215000&v=xD9ttMJ%25mmd2BllWhjGmL2wycveE%25mmd2BUYYbcfYZs5NcrVouWZDZ%25mmd2BBi9lrS0tYEz6qsiJaFr">张梦瑶,朱广丽,张顺香,张标.基于情感分析的微博热点话题用户群体划分模型[J/OL].数据分析与知识发现:1-11[2021-01-22].http://kns.cnki.net/kcms/detail/10.1478.G2.20201216.1154.004.html.</a>

[5]<a id = 'ref5' href = "https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2019&filename=XTYY201903018&v=qeUrHitKYv48OgHRmBdY1MMNwWZGeoRc9xHWuy4hHkGLJYLyY1cXC9nNkyDnP4ix">王杨,王非凡,张舒宜,黄少芬,许闪闪,赵晨曦,赵传信.基于TF-IDF和改进BP神经网络的社交平台垃圾文本过滤[J].计算机系统应用,2019,28(03):126-132.</a>

[6]<a id = 'ref6' href = 'https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2019&filename=JSJY2018S2019&v=aKeECELjBbK%25mmd2FGP7XeXvraq3CGRp%25mmd2B64sjqf7mtz2WkWTTLKVisQthCfYtNT%25mmd2BC3evF'>郝苗苗,徐秀娟,于红,赵小薇,许真珍.基于中文微博的情绪分类与预测算法[J].计算机应用,2018,38(S2):89-96.</a>

[7]<a id = 'ref7' href = 'https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2019&filename=JSJY2018S2019&v=aKeECELjBbK%25mmd2FGP7XeXvraq3CGRp%25mmd2B64sjqf7mtz2WkWTTLKVisQthCfYtNT%25mmd2BC3evF'>郝苗苗,徐秀娟,于红,赵小薇,许真珍.基于中文微博的情绪分类与预测算法[J].计算机应用,2018,38(S2):89-96.</a>

[8]<a id = 'ref8' href = 'https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2019&filename=JSJY2018S2019&v=aKeECELjBbK%25mmd2FGP7XeXvraq3CGRp%25mmd2B64sjqf7mtz2WkWTTLKVisQthCfYtNT%25mmd2BC3evF'>郝苗苗,徐秀娟,于红,赵小薇,许真珍.基于中文微博的情绪分类与预测算法[J].计算机应用,2018,38(S2):89-96.</a>

[9]<a id = 'ref9' herf = "https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CAPJ&dbname=CAPJLAST&filename=XDTQ20201215000&v=xD9ttMJ%25mmd2BllWhjGmL2wycveE%25mmd2BUYYbcfYZs5NcrVouWZDZ%25mmd2BBi9lrS0tYEz6qsiJaFr">张梦瑶,朱广丽,张顺香,张标.基于情感分析的微博热点话题用户群体划分模型[J/OL].数据分析与知识发现:1-11[2021-01-22].http://kns.cnki.net/kcms/detail/10.1478.G2.20201216.1154.004.html.</a>

[10]<a id = 'ref10' herf = "https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CAPJ&dbname=CAPJLAST&filename=XDTQ20201215000&v=xD9ttMJ%25mmd2BllWhjGmL2wycveE%25mmd2BUYYbcfYZs5NcrVouWZDZ%25mmd2BBi9lrS0tYEz6qsiJaFr">张梦瑶,朱广丽,张顺香,张标.基于情感分析的微博热点话题用户群体划分模型[J/OL].数据分析与知识发现:1-11[2021-01-22].http://kns.cnki.net/kcms/detail/10.1478.G2.20201216.1154.004.html.</a>

[11]<a id = 'ref11' herf = "https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CAPJ&dbname=CAPJLAST&filename=XDTQ20201215000&v=xD9ttMJ%25mmd2BllWhjGmL2wycveE%25mmd2BUYYbcfYZs5NcrVouWZDZ%25mmd2BBi9lrS0tYEz6qsiJaFr">张梦瑶,朱广丽,张顺香,张标.基于情感分析的微博热点话题用户群体划分模型[J/OL].数据分析与知识发现:1-11[2021-01-22].http://kns.cnki.net/kcms/detail/10.1478.G2.20201216.1154.004.html.</a>

[12]<a id = 'ref12' herf = "https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CAPJ&dbname=CAPJLAST&filename=XDTQ20201215000&v=xD9ttMJ%25mmd2BllWhjGmL2wycveE%25mmd2BUYYbcfYZs5NcrVouWZDZ%25mmd2BBi9lrS0tYEz6qsiJaFr">张梦瑶,朱广丽,张顺香,张标.基于情感分析的微博热点话题用户群体划分模型[J/OL].数据分析与知识发现:1-11[2021-01-22].http://kns.cnki.net/kcms/detail/10.1478.G2.20201216.1154.004.html.</a>

[13]<a id = 'ref13' href = 'https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFD2007&filename=ZKYB200706008&v=GWTul7ubC3XbHxJsPyJ9k4Jjn%25mmd2FBbmIqNkGa2KxZMApgYPlUHrbK6WLTwUb8UsUoS'>秦钰,荆继武,向继,张爱华.基于优化初始类中心点的K-means改进算法[J].中国科学院研究生院学报,2007(06):771-777.</a>

[14]<a id = 'ref14' href = 'http://www.moj.gov.cn/news/content/2020-02/27/zlk_3242559.html'>http://www.moj.gov.cn/news/content/2020-02/27/zlk_3242559.html</a>

[15]<a id = 'ref15' href = 'https://baijiahao.baidu.com/s?id=1660497999249089790&wfr=spider&for=pc'>https://baijiahao.baidu.com/s?id=1660497999249089790&wfr=spider&for=pc</a>

[16]<a id = 'ref16' href = 'http://hnzs.rmjtxw.com/2020/tbbd_0610/11260.html'>http://hnzs.rmjtxw.com/2020/tbbd_0610/11260.html</a>

<div>
<br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
	<br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
</div>

